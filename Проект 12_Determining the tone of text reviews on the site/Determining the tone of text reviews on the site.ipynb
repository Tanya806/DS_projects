{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Содержание<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Подготовка\" data-toc-modified-id=\"Подготовка-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Подготовка</a></span><ul class=\"toc-item\"><li><span><a href=\"#Загрузка-данных-и-исследование-общей-информации\" data-toc-modified-id=\"Загрузка-данных-и-исследование-общей-информации-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Загрузка данных и исследование общей информации</a></span></li><li><span><a href=\"#Преобразование-текста\" data-toc-modified-id=\"Преобразование-текста-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Преобразование текста</a></span></li><li><span><a href=\"#Балансировка-классов\" data-toc-modified-id=\"Балансировка-классов-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Балансировка классов</a></span></li><li><span><a href=\"#Подготовим-признаки\" data-toc-modified-id=\"Подготовим-признаки-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>Подготовим признаки</a></span></li><li><span><a href=\"#Векторизация-текста-с-помощью-CountVectorizer-(Оценка-важности-слов-/-N-грамм)\" data-toc-modified-id=\"Векторизация-текста-с-помощью-CountVectorizer-(Оценка-важности-слов-/-N-грамм)-1.5\"><span class=\"toc-item-num\">1.5&nbsp;&nbsp;</span>Векторизация текста с помощью CountVectorizer (Оценка важности слов / N-грамм)</a></span></li><li><span><a href=\"#TF-IDF\" data-toc-modified-id=\"TF-IDF-1.6\"><span class=\"toc-item-num\">1.6&nbsp;&nbsp;</span>TF-IDF</a></span></li></ul></li><li><span><a href=\"#Обучение\" data-toc-modified-id=\"Обучение-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Обучение</a></span><ul class=\"toc-item\"><li><span><a href=\"#Логистическая-регрессия\" data-toc-modified-id=\"Логистическая-регрессия-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Логистическая регрессия</a></span></li><li><span><a href=\"#Деревья-поиска\" data-toc-modified-id=\"Деревья-поиска-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Деревья поиска</a></span></li></ul></li><li><span><a href=\"#Выводы\" data-toc-modified-id=\"Выводы-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Выводы</a></span></li><li><span><a href=\"#Чек-лист-проверки\" data-toc-modified-id=\"Чек-лист-проверки-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Чек-лист проверки</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Проект для «Викишоп»"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Интернет-магазин «Викишоп» запускает новый сервис. Теперь пользователи могут редактировать и дополнять описания товаров, как в вики-сообществах. То есть клиенты предлагают свои правки и комментируют изменения других. Магазину нужен инструмент, который будет искать токсичные комментарии и отправлять их на модерацию. \n",
    "\n",
    "Обучите модель классифицировать комментарии на позитивные и негативные. В вашем распоряжении набор данных с разметкой о токсичности правок.\n",
    "\n",
    "Постройте модель со значением метрики качества *F1* не меньше 0.75. \n",
    "\n",
    "**Инструкция по выполнению проекта**\n",
    "\n",
    "1. Загрузите и подготовьте данные.\n",
    "2. Обучите разные модели. \n",
    "3. Сделайте выводы.\n",
    "\n",
    "Для выполнения проекта применять *BERT* необязательно, но вы можете попробовать.\n",
    "\n",
    "**Описание данных**\n",
    "\n",
    "Данные находятся в файле `toxic_comments.csv`. Столбец *text* в нём содержит текст комментария, а *toxic* — целевой признак."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подготовка"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Загрузка данных и исследование общей информации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in /opt/conda/lib/python3.9/site-packages (3.2.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/conda/lib/python3.9/site-packages (from spacy) (2.25.1)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /opt/conda/lib/python3.9/site-packages (from spacy) (0.10.1)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.9/site-packages (from spacy) (49.6.0.post20210108)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in /opt/conda/lib/python3.9/site-packages (from spacy) (3.0.10)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/conda/lib/python3.9/site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /opt/conda/lib/python3.9/site-packages (from spacy) (0.4.2)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.12 in /opt/conda/lib/python3.9/site-packages (from spacy) (8.0.17)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.9/site-packages (from spacy) (21.3)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /opt/conda/lib/python3.9/site-packages (from spacy) (0.6.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /opt/conda/lib/python3.9/site-packages (from spacy) (2.4.4)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /opt/conda/lib/python3.9/site-packages (from spacy) (0.7.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/conda/lib/python3.9/site-packages (from spacy) (3.0.7)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /opt/conda/lib/python3.9/site-packages (from spacy) (1.8.2)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/lib/python3.9/site-packages (from spacy) (2.0.6)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/conda/lib/python3.9/site-packages (from spacy) (4.61.2)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.9/site-packages (from spacy) (3.0.1)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/conda/lib/python3.9/site-packages (from spacy) (1.0.3)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /opt/conda/lib/python3.9/site-packages (from spacy) (1.21.1)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/lib/python3.9/site-packages (from spacy) (1.0.8)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/conda/lib/python3.9/site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.9/site-packages (from packaging>=20.0->spacy) (2.4.7)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /opt/conda/lib/python3.9/site-packages (from pathy>=0.3.5->spacy) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy) (4.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.6.15)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (4.0.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.6)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /opt/conda/lib/python3.9/site-packages (from typer<0.5.0,>=0.3.0->spacy) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.9/site-packages (from jinja2->spacy) (2.1.1)\n",
      "Collecting en-core-web-sm==3.2.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.2.0/en_core_web_sm-3.2.0-py3-none-any.whl (13.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 13.9 MB 1.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.3.0,>=3.2.0 in /opt/conda/lib/python3.9/site-packages (from en-core-web-sm==3.2.0) (3.2.0)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /opt/conda/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.6.2)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.0.8)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /opt/conda/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.4.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (21.3)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.12 in /opt/conda/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (8.0.17)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in /opt/conda/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.10)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (49.6.0.post20210108)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/conda/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.3.0)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/conda/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/conda/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.7)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /opt/conda/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.4.4)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /opt/conda/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.7.8)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.6)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/conda/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (4.61.2)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/conda/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.0.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/conda/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.25.1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /opt/conda/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.21.1)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /opt/conda/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.10.1)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /opt/conda/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.8.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.9/site-packages (from packaging>=20.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.4.7)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /opt/conda/lib/python3.9/site-packages (from pathy>=0.3.5->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (4.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2022.6.15)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.26.6)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.10)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /opt/conda/lib/python3.9/site-packages (from typer<0.5.0,>=0.3.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.9/site-packages (from jinja2->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.1.1)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "# !pip uninstall spacy\n",
    "!pip install spacy\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.corpus import stopwords as nltk_stopwords\n",
    "from nltk.stem import WordNetLemmatizer #для английского текста\n",
    "#import re\n",
    "import string\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "nlp =  spacy.load(\"en_core_web_sm\")\n",
    "#nlp = en_core_web_sm.load() #для русского - ru_core_news_sm\n",
    "#data_comments = pd.read_csv(\"/Users/tatiana/Downloads/toxic_comments.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    " data_comments = pd.read_csv(\"/datasets/toxic_comments.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "Библиотеки импортированы и данные загружены\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                               text  toxic\n",
       "0           0  Explanation\\nWhy the edits made under my usern...      0\n",
       "1           1  D'aww! He matches this background colour I'm s...      0\n",
       "2           2  Hey man, I'm really not trying to edit war. It...      0\n",
       "3           3  \"\\nMore\\nI can't make any real suggestions on ...      0\n",
       "4           4  You, sir, are my hero. Any chance you remember...      0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>159292.000000</td>\n",
       "      <td>159292.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>79725.697242</td>\n",
       "      <td>0.101612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>46028.837471</td>\n",
       "      <td>0.302139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>39872.750000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>79721.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>119573.250000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>159450.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Unnamed: 0          toxic\n",
       "count  159292.000000  159292.000000\n",
       "mean    79725.697242       0.101612\n",
       "std     46028.837471       0.302139\n",
       "min         0.000000       0.000000\n",
       "25%     39872.750000       0.000000\n",
       "50%     79721.500000       0.000000\n",
       "75%    119573.250000       0.000000\n",
       "max    159450.000000       1.000000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 159292 entries, 0 to 159291\n",
      "Data columns (total 3 columns):\n",
      " #   Column      Non-Null Count   Dtype \n",
      "---  ------      --------------   ----- \n",
      " 0   Unnamed: 0  159292 non-null  int64 \n",
      " 1   text        159292 non-null  object\n",
      " 2   toxic       159292 non-null  int64 \n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 3.6+ MB\n"
     ]
    }
   ],
   "source": [
    "display(data_comments.head(5))\n",
    "display(data_comments.describe())\n",
    "data_comments.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unnamed: 0    0\n",
      "text          0\n",
      "toxic         0\n",
      "dtype: int64\n",
      "Число полных дубликатов строк в таблице: 0\n"
     ]
    }
   ],
   "source": [
    "print(data_comments.isnull().sum())\n",
    "print(\"Число полных дубликатов строк в таблице:\", data_comments.duplicated().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "Дубликаты и нулевые строки отсутствуют, перед нами стоит задача бинарной классификации\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Преобразование текста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Очищенный текст:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>clear_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>Explanation Why the edits made under my userna...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>D aww He matches this background colour I m se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>Hey man I m really not trying to edit war It s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>More I can t make any real suggestions on impr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>You sir are my hero Any chance you remember wh...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                               text  toxic  \\\n",
       "0           0  Explanation\\nWhy the edits made under my usern...      0   \n",
       "1           1  D'aww! He matches this background colour I'm s...      0   \n",
       "2           2  Hey man, I'm really not trying to edit war. It...      0   \n",
       "3           3  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
       "4           4  You, sir, are my hero. Any chance you remember...      0   \n",
       "\n",
       "                                          clear_text  \n",
       "0  Explanation Why the edits made under my userna...  \n",
       "1  D aww He matches this background colour I m se...  \n",
       "2  Hey man I m really not trying to edit war It s...  \n",
       "3  More I can t make any real suggestions on impr...  \n",
       "4  You sir are my hero Any chance you remember wh...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#1  Удалим лишние символы\n",
    "import re\n",
    "def clear_text(text):\n",
    "    clear_text = re.sub(r'[^a-zA-Z ]', ' ', text) \n",
    "    clear_text = clear_text.split()\n",
    "    clear_text = \" \".join(clear_text)\n",
    "    return clear_text\n",
    "\n",
    "data_comments['clear_text'] = data_comments['text'].apply(lambda x: clear_text(x))\n",
    "print(\"Очищенный текст:\")\n",
    "display(data_comments.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Текст после токенизации:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>clear_text</th>\n",
       "      <th>tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>Explanation Why the edits made under my userna...</td>\n",
       "      <td>[explanation, why, the, edits, made, under, my...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>D aww He matches this background colour I m se...</td>\n",
       "      <td>[d, aww, he, matches, this, background, colour...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>Hey man I m really not trying to edit war It s...</td>\n",
       "      <td>[hey, man, i, m, really, not, trying, to, edit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>More I can t make any real suggestions on impr...</td>\n",
       "      <td>[more, i, can, t, make, any, real, suggestions...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>You sir are my hero Any chance you remember wh...</td>\n",
       "      <td>[you, sir, are, my, hero, any, chance, you, re...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                               text  toxic  \\\n",
       "0           0  Explanation\\nWhy the edits made under my usern...      0   \n",
       "1           1  D'aww! He matches this background colour I'm s...      0   \n",
       "2           2  Hey man, I'm really not trying to edit war. It...      0   \n",
       "3           3  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
       "4           4  You, sir, are my hero. Any chance you remember...      0   \n",
       "\n",
       "                                          clear_text  \\\n",
       "0  Explanation Why the edits made under my userna...   \n",
       "1  D aww He matches this background colour I m se...   \n",
       "2  Hey man I m really not trying to edit war It s...   \n",
       "3  More I can t make any real suggestions on impr...   \n",
       "4  You sir are my hero Any chance you remember wh...   \n",
       "\n",
       "                                           tokenized  \n",
       "0  [explanation, why, the, edits, made, under, my...  \n",
       "1  [d, aww, he, matches, this, background, colour...  \n",
       "2  [hey, man, i, m, really, not, trying, to, edit...  \n",
       "3  [more, i, can, t, make, any, real, suggestions...  \n",
       "4  [you, sir, are, my, hero, any, chance, you, re...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#2 Токенизация\n",
    "def tokenization(text):\n",
    "    text = re.split('\\W+', text)\n",
    "    return text\n",
    "\n",
    "data_comments['tokenized'] = data_comments['clear_text'].apply(lambda x: tokenization(x.lower()))\n",
    "print(\"Текст после токенизации:\")\n",
    "display(data_comments.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Загружаем английские стоп-слова\n",
    "nltk.download('stopwords')\n",
    "stopwords = set(nltk_stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Текст после удаления стоп-слов:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>clear_text</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>nonstop</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>Explanation Why the edits made under my userna...</td>\n",
       "      <td>[explanation, why, the, edits, made, under, my...</td>\n",
       "      <td>explanation edits made username hardcore metal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>D aww He matches this background colour I m se...</td>\n",
       "      <td>[d, aww, he, matches, this, background, colour...</td>\n",
       "      <td>aww matches background colour seemingly stuck ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>Hey man I m really not trying to edit war It s...</td>\n",
       "      <td>[hey, man, i, m, really, not, trying, to, edit...</td>\n",
       "      <td>hey man really trying edit war guy constantly ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>More I can t make any real suggestions on impr...</td>\n",
       "      <td>[more, i, can, t, make, any, real, suggestions...</td>\n",
       "      <td>make real suggestions improvement wondered sec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>You sir are my hero Any chance you remember wh...</td>\n",
       "      <td>[you, sir, are, my, hero, any, chance, you, re...</td>\n",
       "      <td>sir hero chance remember page</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                               text  toxic  \\\n",
       "0           0  Explanation\\nWhy the edits made under my usern...      0   \n",
       "1           1  D'aww! He matches this background colour I'm s...      0   \n",
       "2           2  Hey man, I'm really not trying to edit war. It...      0   \n",
       "3           3  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
       "4           4  You, sir, are my hero. Any chance you remember...      0   \n",
       "\n",
       "                                          clear_text  \\\n",
       "0  Explanation Why the edits made under my userna...   \n",
       "1  D aww He matches this background colour I m se...   \n",
       "2  Hey man I m really not trying to edit war It s...   \n",
       "3  More I can t make any real suggestions on impr...   \n",
       "4  You sir are my hero Any chance you remember wh...   \n",
       "\n",
       "                                           tokenized  \\\n",
       "0  [explanation, why, the, edits, made, under, my...   \n",
       "1  [d, aww, he, matches, this, background, colour...   \n",
       "2  [hey, man, i, m, really, not, trying, to, edit...   \n",
       "3  [more, i, can, t, make, any, real, suggestions...   \n",
       "4  [you, sir, are, my, hero, any, chance, you, re...   \n",
       "\n",
       "                                             nonstop  \n",
       "0  explanation edits made username hardcore metal...  \n",
       "1  aww matches background colour seemingly stuck ...  \n",
       "2  hey man really trying edit war guy constantly ...  \n",
       "3  make real suggestions improvement wondered sec...  \n",
       "4                      sir hero chance remember page  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#3 Удаление стоп-слов\n",
    "def remove_stopwords(text):\n",
    "    text = [word for word in text if word not in stopwords]\n",
    "    text = \" \".join(text)\n",
    "    return text\n",
    "    \n",
    "data_comments['nonstop'] = data_comments['tokenized'].apply(lambda x: remove_stopwords(x))\n",
    "print(\"Текст после удаления стоп-слов:\")\n",
    "display(data_comments.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4 Лемматизация\n",
    "#lemm = nltk.WordNetLemmatizer()\n",
    "def lemmatize(text):\n",
    "    doc = nlp(text)\n",
    "    lemm_text = \" \".join([token.lemma_ if token.lemma_ !='-PRON-' else token.text for token in doc])      \n",
    "    return lemm_text\n",
    "\n",
    "data_comments['lemmatized'] = data_comments['nonstop'].apply(lambda x: lemmatize(x))\n",
    "print(\"Текст после лемматизации:\")\n",
    "display(data_comments.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "Интересная статья на тему нормализации текстов:  https://towardsdatascience.com/text-normalization-7ecc8e084e31\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Балансировка классов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x = 'toxic', data = data_comments)\n",
    "\n",
    "toxic_numbers = data_comments['toxic'].value_counts()\n",
    "toxic_rate = toxic_numbers[1] / toxic_numbers[0] \n",
    "print('Распределение комментариев на негативные и позитивные: {:.2f}'.format(toxic_rate))\n",
    "print(toxic_numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Upsampling: cделаем объекты редкого класса не такими редкими\n",
    "def upsample(features, target, repeat):\n",
    "    features_zeros = features[target == 0]\n",
    "    features_ones = features[target == 1]\n",
    "    target_zeros = target[target == 0]\n",
    "    target_ones = target[target == 1]\n",
    "\n",
    "    features_upsampled = pd.concat([features_zeros] + [features_ones] * repeat)\n",
    "    target_upsampled = pd.concat([target_zeros] + [target_ones] * repeat)\n",
    "    \n",
    "    features_upsampled, target_upsampled = shuffle(\n",
    "        features_upsampled, target_upsampled, random_state=12345)\n",
    "    \n",
    "    return features_upsampled, target_upsampled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "Произвела балансировку классов перед моделированием. \n",
    "Для решения проблемы дисбаланса классов можно посмотреть: https://www.analyticsvidhya.com/blog/2020/10/overcoming-class-imbalance-using-smote-techniques/. SMOTE очень распространен в реальной жизни и исследованиях.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Подготовим признаки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = data_comments.drop(['toxic'], axis = 1)\n",
    "target = data_comments['toxic']\n",
    "\n",
    "print(\"Размер матрицы признаков:\", features.shape)\n",
    "print(\"Размер целевого признака:\", target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Выделим 80% исходные данных для обучаюшей выборки, и 20% данных для тестовой выборки\n",
    "features_train, features_test, target_train, target_test = train_test_split(\n",
    "    features, target, test_size=0.2, random_state=12345)\n",
    "\n",
    "# Проверка размеров полученных выборок\n",
    "print('Размер полученных выборок:')\n",
    "print('features_train:', features_train.shape, 'features_test:', features_test.shape)\n",
    "print('target_train:', target_train.shape, 'target_test:', target_test.shape)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Используем метод upsampling для улучшения дисбаланса классов в обучаюшей выборке\n",
    "upsample_repeat = round(1/toxic_rate)\n",
    "print(\"upsample_repeat\", upsample_repeat)\n",
    "features_train, target_train = upsample(features_train, target_train, upsample_repeat)\n",
    "\n",
    "# Проверка размеров получившихся обучающих выборок\n",
    "print(\"Размер обучающих выборок после балансировки классов:\")\n",
    "print(\"features_train:\", features_train.shape, \"target_train:\", target_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Векторизация текста с помощью CountVectorizer (Оценка важности слов / N-грамм)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_train = features_train['lemmatized'].values.astype('U')   \n",
    "features_train_cv = count_vect.fit_transform(corpus_train) \n",
    "\n",
    "print(\"Размер матрицы features_train (CountVectorizer):\", features_train_cv.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_test = features_test['lemmatized'].values.astype('U')     #  Преобразовываем англ. текст к юникоду\n",
    "features_test_cv = count_vect.transform(corpus_test)\n",
    "\n",
    "print(\"Размер матрицы features_test (CountVectorizer):\", features_test_cv.shape)\n",
    "print(features_test_cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Переведем тесты из твиттера в векторный формат с помощью TF-IDF меры. В этой модели вес некоторого слова пропорционален частоте употребления этого слова в документе и обратно пропорционален частоте употребления слова во всех документах коллекции"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_tf_idf = TfidfVectorizer(stop_words=stopwords)\n",
    "\n",
    "# Матрица TF-IDF на обучающей выборке\n",
    "corpus_train = features_train['lemmatized'].values.astype('U')\n",
    "features_train_tf_idf = count_tf_idf.fit_transform(corpus_train)\n",
    "\n",
    "print(\"Размер матрицы features_train (TF-IDF):\", features_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Матрица TF-IDF на тестовой выборке\n",
    "corpus_test = features_test['lemmatized'].values.astype('U')\n",
    "features_test_tf_idf = count_tf_idf.transform(corpus_test)\n",
    "\n",
    "print(\"Размер матрицы features_test (TF-IDF):\", features_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Логистическая регрессия"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_lr = [{'solver': ['liblinear','newton-cg', 'sag', 'saga', 'lbfgs']}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Убираем вывод Warnings \n",
    "if not sys.warnoptions:\n",
    "       warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "reg_model = LogisticRegression()\n",
    "grid_search = GridSearchCV(estimator=reg_model, param_grid=param_grid_lr, scoring='f1', cv=5)\n",
    "grid_search.fit(features_train_cv, target_train)\n",
    "\n",
    "print(\"Подобраны гиперпараметры модели логистической регрессии:\")\n",
    "print(grid_search.best_params_) \n",
    "#CPU times: user 45min 21s, sys: 10min 32s, total: 55min 53s\n",
    "#Wall time: 9min 2s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(solver='newton-cg', random_state=12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Обучим модель логистической регрессии для CountVectorizer \n",
    "model.fit(features_train_cv, target_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = model.predict(features_test_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_lr_cv = f1_score(target_test, predicted)\n",
    "print(\"Метрика F1 для CountVectorizer):\", f1_lr_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Обучим модель для TfidfVectorizer\n",
    "model.fit(features_train_tf_idf, target_train)\n",
    "predicted = model.predict(features_test_tf_idf) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_lr_tf_idf = f1_score(target_test, predicted)\n",
    "print(\"Метрика F1 для TfidfVectorizer: \", f1_lr_tf_idf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Деревья поиска"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#DecisionTree\n",
    "model_DecisionTree = DecisionTreeClassifier(random_state=12345)\n",
    "\n",
    "tree_params = {'max_depth': range(11,19,2), 'max_features': range(6,15,2),'random_state': [12345]}\n",
    "\n",
    "grid_search = GridSearchCV(model_DecisionTree, param_grid=tree_params, scoring='f1', cv=5)\n",
    "grid_search.fit(features_train_cv, target_train)\n",
    "print(\"Подобраны гиперпараметры модели дерева решений:\")\n",
    "print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Модель дерева решений с подобранными гиперпараметрами\n",
    "model_tree = DecisionTreeClassifier(max_depth = 17, max_features = 12, random_state=12345)\n",
    "model_tree.fit(features_train_cv, target_train)\n",
    "\n",
    "predicted = model_tree.predict(features_test_cv)\n",
    "f1_tree_cv = f1_score(target_test, predicted)\n",
    "print(\"Метрика F1 для CountVectorizer:\", f1_tree_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Обучим модель для TfidfVectorizer\n",
    "model_tree.fit(features_train_tf_idf, target_train)\n",
    "predicted = model_tree.predict(features_test_tf_idf) \n",
    "f1_tree_tf_idf = f1_score(target_test, predicted)\n",
    "print(\"Метрика F1 для TfidfVectorizer: \", f1_tree_tf_idf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Выводы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для выбора лучшей модели выявления токсичных комментариев к описаниям товаров интернет-магазина была проделана работа:\n",
    "\n",
    "    1. загрузили данные о комментариях покупателей \n",
    "    2. Выполнили обработку данных\n",
    "    3. Лемматизировали тексты\n",
    "    4. Рассчитали частоту употребления слов/N-грамм в комментариях методами CountVectorizer (\"мешка слов\") и TF-IDF.\n",
    "    5. Обучили модели логистической регрессии и дерева поиска\n",
    "    6. Выбрали модель с лучшим значением метрики F1.\n",
    "    \n",
    "Лучшее качество по метрике F1 дает модель логистической регрессии с методом оценки частоты употребления слов CountVectorizer - 0.75."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Чек-лист проверки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [x]  Jupyter Notebook открыт\n",
    "- [x]  Весь код выполняется без ошибок\n",
    "- [x]  Ячейки с кодом расположены в порядке исполнения\n",
    "- [x]  Данные загружены и подготовлены\n",
    "- [x]  Модели обучены\n",
    "- [x]  Значение метрики *F1* не меньше 0.75\n",
    "- [x]  Выводы написаны"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "Полезная статья на будущее: 24 метрики по проверке адекватности модели бинарной классификации: https://neptune.ai/blog/evaluation-metrics-binary-classification. \n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "ExecuteTimeLog": [
   {
    "duration": 11255,
    "start_time": "2022-02-19T20:49:56.892Z"
   },
   {
    "duration": 71,
    "start_time": "2022-02-19T20:50:46.076Z"
   },
   {
    "duration": 31,
    "start_time": "2022-02-19T20:51:56.734Z"
   },
   {
    "duration": 468,
    "start_time": "2022-02-19T20:53:23.266Z"
   },
   {
    "duration": 329,
    "start_time": "2022-02-19T20:53:30.657Z"
   },
   {
    "duration": 1620,
    "start_time": "2022-02-19T20:56:01.206Z"
   },
   {
    "duration": 55,
    "start_time": "2022-02-19T20:56:02.830Z"
   },
   {
    "duration": 339,
    "start_time": "2022-02-19T20:56:03.202Z"
   },
   {
    "duration": 23313,
    "start_time": "2022-02-19T20:59:16.568Z"
   },
   {
    "duration": 406,
    "start_time": "2022-02-19T21:14:49.720Z"
   },
   {
    "duration": 68463,
    "start_time": "2022-02-19T21:18:00.404Z"
   },
   {
    "duration": 578,
    "start_time": "2022-02-19T21:19:15.185Z"
   },
   {
    "duration": 1838,
    "start_time": "2022-02-19T21:20:09.026Z"
   },
   {
    "duration": 350,
    "start_time": "2022-02-19T21:20:27.638Z"
   },
   {
    "duration": 11591,
    "start_time": "2022-02-19T21:20:32.683Z"
   },
   {
    "duration": 72,
    "start_time": "2022-02-19T21:20:44.276Z"
   },
   {
    "duration": 343,
    "start_time": "2022-02-19T21:20:44.352Z"
   },
   {
    "duration": 4163,
    "start_time": "2022-02-19T21:20:44.699Z"
   },
   {
    "duration": 815,
    "start_time": "2022-02-19T21:21:28.473Z"
   },
   {
    "duration": 12011,
    "start_time": "2022-02-19T21:24:38.232Z"
   },
   {
    "duration": 75,
    "start_time": "2022-02-19T21:24:50.246Z"
   },
   {
    "duration": 341,
    "start_time": "2022-02-19T21:24:50.324Z"
   },
   {
    "duration": 3694,
    "start_time": "2022-02-19T21:24:50.667Z"
   },
   {
    "duration": 1166,
    "start_time": "2022-02-19T21:34:06.140Z"
   },
   {
    "duration": 11203,
    "start_time": "2022-02-19T21:34:13.662Z"
   },
   {
    "duration": 75,
    "start_time": "2022-02-19T21:34:24.867Z"
   },
   {
    "duration": 332,
    "start_time": "2022-02-19T21:34:24.945Z"
   },
   {
    "duration": 2956,
    "start_time": "2022-02-19T21:34:25.279Z"
   },
   {
    "duration": 14485,
    "start_time": "2022-02-19T21:34:28.238Z"
   },
   {
    "duration": 12755,
    "start_time": "2022-02-19T21:36:50.148Z"
   },
   {
    "duration": 28,
    "start_time": "2022-02-19T21:37:53.897Z"
   },
   {
    "duration": 158167,
    "start_time": "2022-02-19T21:38:02.299Z"
   },
   {
    "duration": 3038,
    "start_time": "2022-02-20T12:46:45.111Z"
   },
   {
    "duration": 74,
    "start_time": "2022-02-20T12:46:48.152Z"
   },
   {
    "duration": 340,
    "start_time": "2022-02-20T12:46:48.229Z"
   },
   {
    "duration": 9,
    "start_time": "2022-02-20T12:46:50.096Z"
   },
   {
    "duration": 12720,
    "start_time": "2022-02-20T12:46:51.116Z"
   },
   {
    "duration": 6,
    "start_time": "2022-02-20T12:48:17.392Z"
   },
   {
    "duration": 2946,
    "start_time": "2022-02-20T12:48:18.239Z"
   },
   {
    "duration": 124023,
    "start_time": "2022-02-20T12:48:23.981Z"
   },
   {
    "duration": 362,
    "start_time": "2022-02-20T13:12:18.552Z"
   },
   {
    "duration": 11,
    "start_time": "2022-02-20T13:18:37.325Z"
   },
   {
    "duration": 504,
    "start_time": "2022-02-20T13:24:12.330Z"
   },
   {
    "duration": 22,
    "start_time": "2022-02-20T13:24:37.957Z"
   },
   {
    "duration": 279,
    "start_time": "2022-02-20T13:27:32.227Z"
   },
   {
    "duration": 298,
    "start_time": "2022-02-20T13:27:43.961Z"
   },
   {
    "duration": 1355,
    "start_time": "2022-02-20T13:28:57.028Z"
   },
   {
    "duration": 53,
    "start_time": "2022-02-20T13:28:58.934Z"
   },
   {
    "duration": 302,
    "start_time": "2022-02-20T13:28:59.850Z"
   },
   {
    "duration": 6,
    "start_time": "2022-02-20T13:29:01.082Z"
   },
   {
    "duration": 3371,
    "start_time": "2022-02-20T13:29:02.691Z"
   },
   {
    "duration": 50,
    "start_time": "2022-02-20T13:47:25.899Z"
   },
   {
    "duration": 466,
    "start_time": "2022-02-20T13:53:34.516Z"
   },
   {
    "duration": 6,
    "start_time": "2022-02-20T13:53:43.140Z"
   },
   {
    "duration": 9,
    "start_time": "2022-02-20T13:53:47.792Z"
   },
   {
    "duration": 6,
    "start_time": "2022-02-20T13:53:48.576Z"
   },
   {
    "duration": 48,
    "start_time": "2022-02-20T13:53:51.255Z"
   },
   {
    "duration": 108,
    "start_time": "2022-02-20T13:53:55.132Z"
   },
   {
    "duration": 190,
    "start_time": "2022-02-20T13:54:00.711Z"
   },
   {
    "duration": 266,
    "start_time": "2022-02-20T13:58:08.437Z"
   },
   {
    "duration": 322,
    "start_time": "2022-02-20T15:03:59.329Z"
   },
   {
    "duration": 928,
    "start_time": "2022-02-20T15:04:10.648Z"
   },
   {
    "duration": 241,
    "start_time": "2022-02-20T15:04:19.236Z"
   },
   {
    "duration": 308,
    "start_time": "2022-02-20T15:04:50.412Z"
   },
   {
    "duration": 59,
    "start_time": "2022-02-20T15:05:19.237Z"
   },
   {
    "duration": 339,
    "start_time": "2022-02-20T15:05:19.896Z"
   },
   {
    "duration": 7,
    "start_time": "2022-02-20T15:05:20.917Z"
   },
   {
    "duration": 3408,
    "start_time": "2022-02-20T15:05:21.333Z"
   },
   {
    "duration": 192,
    "start_time": "2022-02-20T15:05:24.744Z"
   },
   {
    "duration": 359,
    "start_time": "2022-02-20T15:05:24.939Z"
   },
   {
    "duration": 9,
    "start_time": "2022-02-20T15:05:30.533Z"
   },
   {
    "duration": 53,
    "start_time": "2022-02-20T15:05:31.799Z"
   },
   {
    "duration": 157,
    "start_time": "2022-02-20T15:05:32.748Z"
   },
   {
    "duration": 150,
    "start_time": "2022-02-20T15:05:33.956Z"
   },
   {
    "duration": 282,
    "start_time": "2022-02-20T15:05:38.608Z"
   },
   {
    "duration": 4,
    "start_time": "2022-02-20T15:06:14.812Z"
   },
   {
    "duration": 340,
    "start_time": "2022-02-20T15:18:37.013Z"
   },
   {
    "duration": 790,
    "start_time": "2022-02-20T15:18:45.509Z"
   },
   {
    "duration": 57,
    "start_time": "2022-02-20T15:18:46.868Z"
   },
   {
    "duration": 311,
    "start_time": "2022-02-20T15:18:47.417Z"
   },
   {
    "duration": 6,
    "start_time": "2022-02-20T15:18:48.305Z"
   },
   {
    "duration": 3192,
    "start_time": "2022-02-20T15:18:48.709Z"
   },
   {
    "duration": 5776,
    "start_time": "2022-02-20T15:19:34.585Z"
   },
   {
    "duration": 3347,
    "start_time": "2022-02-20T15:20:43.329Z"
   },
   {
    "duration": 3445,
    "start_time": "2022-02-20T15:20:50.805Z"
   },
   {
    "duration": 5,
    "start_time": "2022-02-20T15:22:06.069Z"
   },
   {
    "duration": 3340,
    "start_time": "2022-02-20T15:22:06.789Z"
   },
   {
    "duration": 3559,
    "start_time": "2022-02-20T15:22:11.910Z"
   },
   {
    "duration": 8,
    "start_time": "2022-02-20T15:26:26.938Z"
   },
   {
    "duration": 4023,
    "start_time": "2022-02-20T15:26:31.924Z"
   },
   {
    "duration": 70773,
    "start_time": "2022-02-20T15:26:37.129Z"
   },
   {
    "duration": 6,
    "start_time": "2022-02-20T15:28:58.209Z"
   },
   {
    "duration": 3942,
    "start_time": "2022-02-20T15:29:02.845Z"
   },
   {
    "duration": 2555,
    "start_time": "2022-02-20T15:29:07.849Z"
   },
   {
    "duration": 7,
    "start_time": "2022-02-20T15:30:53.811Z"
   },
   {
    "duration": 3358,
    "start_time": "2022-02-20T15:30:59.177Z"
   },
   {
    "duration": 2460,
    "start_time": "2022-02-20T15:31:03.976Z"
   },
   {
    "duration": 8,
    "start_time": "2022-02-20T15:32:52.876Z"
   },
   {
    "duration": 3440,
    "start_time": "2022-02-20T15:33:00.077Z"
   },
   {
    "duration": 70749,
    "start_time": "2022-02-20T15:33:20.650Z"
   },
   {
    "duration": 6,
    "start_time": "2022-02-20T15:36:15.644Z"
   },
   {
    "duration": 3171,
    "start_time": "2022-02-20T15:36:21.378Z"
   },
   {
    "duration": 6,
    "start_time": "2022-02-20T15:36:24.553Z"
   },
   {
    "duration": 65589,
    "start_time": "2022-02-20T15:36:24.563Z"
   },
   {
    "duration": 6,
    "start_time": "2022-02-20T15:41:52.148Z"
   },
   {
    "duration": 2922,
    "start_time": "2022-02-20T15:41:56.537Z"
   },
   {
    "duration": 134099,
    "start_time": "2022-02-20T15:42:03.796Z"
   },
   {
    "duration": 921,
    "start_time": "2022-02-20T16:01:28.923Z"
   },
   {
    "duration": 69,
    "start_time": "2022-02-20T16:01:29.848Z"
   },
   {
    "duration": 340,
    "start_time": "2022-02-20T16:01:30.296Z"
   },
   {
    "duration": 3313,
    "start_time": "2022-02-20T16:01:31.839Z"
   },
   {
    "duration": 6786,
    "start_time": "2022-02-20T16:01:35.155Z"
   },
   {
    "duration": 65521,
    "start_time": "2022-02-20T16:05:57.324Z"
   },
   {
    "duration": 1490,
    "start_time": "2022-02-20T16:19:12.910Z"
   },
   {
    "duration": 56,
    "start_time": "2022-02-20T16:19:14.405Z"
   },
   {
    "duration": 351,
    "start_time": "2022-02-20T16:19:14.464Z"
   },
   {
    "duration": 3020,
    "start_time": "2022-02-20T16:19:15.355Z"
   },
   {
    "duration": 6551,
    "start_time": "2022-02-20T16:19:18.378Z"
   },
   {
    "duration": 198,
    "start_time": "2022-02-20T16:20:30.149Z"
   },
   {
    "duration": 13,
    "start_time": "2022-02-20T16:20:45.800Z"
   },
   {
    "duration": 64536,
    "start_time": "2022-02-20T16:20:47.666Z"
   },
   {
    "duration": 296,
    "start_time": "2022-02-20T16:24:53.904Z"
   },
   {
    "duration": 1745,
    "start_time": "2022-02-20T16:25:06.824Z"
   },
   {
    "duration": 2213,
    "start_time": "2022-02-20T16:25:31.543Z"
   },
   {
    "duration": 36295,
    "start_time": "2022-02-20T16:25:38.728Z"
   },
   {
    "duration": 251,
    "start_time": "2022-02-20T16:26:25.907Z"
   },
   {
    "duration": 6,
    "start_time": "2022-02-20T16:26:29.740Z"
   },
   {
    "duration": 4,
    "start_time": "2022-02-20T16:27:50.511Z"
   },
   {
    "duration": 46,
    "start_time": "2022-10-07T11:42:16.360Z"
   },
   {
    "duration": 6,
    "start_time": "2022-10-07T11:42:16.856Z"
   },
   {
    "duration": 14135,
    "start_time": "2022-10-07T11:42:22.443Z"
   },
   {
    "duration": 4956,
    "start_time": "2022-10-07T11:42:36.581Z"
   },
   {
    "duration": 0,
    "start_time": "2022-10-07T11:42:41.539Z"
   },
   {
    "duration": 0,
    "start_time": "2022-10-07T11:42:41.540Z"
   },
   {
    "duration": 16,
    "start_time": "2022-10-07T11:42:44.243Z"
   },
   {
    "duration": 10843,
    "start_time": "2022-10-07T11:42:56.363Z"
   },
   {
    "duration": 453,
    "start_time": "2022-10-07T11:43:07.209Z"
   },
   {
    "duration": 0,
    "start_time": "2022-10-07T11:43:07.664Z"
   },
   {
    "duration": 0,
    "start_time": "2022-10-07T11:43:07.665Z"
   },
   {
    "duration": 17,
    "start_time": "2022-10-07T11:43:09.175Z"
   },
   {
    "duration": 15,
    "start_time": "2022-10-07T11:43:19.224Z"
   },
   {
    "duration": 17,
    "start_time": "2022-10-07T11:43:21.748Z"
   },
   {
    "duration": 441,
    "start_time": "2022-10-07T11:44:38.087Z"
   },
   {
    "duration": 642,
    "start_time": "2022-10-07T11:44:47.023Z"
   },
   {
    "duration": 418,
    "start_time": "2022-10-07T11:46:13.731Z"
   },
   {
    "duration": 392,
    "start_time": "2022-10-07T11:46:27.171Z"
   },
   {
    "duration": 23,
    "start_time": "2022-10-07T11:46:48.115Z"
   },
   {
    "duration": 21,
    "start_time": "2022-10-07T11:46:57.264Z"
   },
   {
    "duration": 2214,
    "start_time": "2022-10-07T11:48:48.271Z"
   },
   {
    "duration": 57,
    "start_time": "2022-10-07T11:48:57.283Z"
   },
   {
    "duration": 222,
    "start_time": "2022-10-07T11:48:58.363Z"
   },
   {
    "duration": 396,
    "start_time": "2022-10-07T11:49:00.423Z"
   },
   {
    "duration": 1780,
    "start_time": "2022-10-07T11:50:18.947Z"
   },
   {
    "duration": 239,
    "start_time": "2022-10-07T11:50:22.164Z"
   },
   {
    "duration": 3770,
    "start_time": "2022-10-07T11:51:09.963Z"
   },
   {
    "duration": 191,
    "start_time": "2022-10-07T11:51:16.663Z"
   },
   {
    "duration": 980,
    "start_time": "2022-10-07T11:51:21.035Z"
   }
  ],
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Содержание",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "302.391px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
